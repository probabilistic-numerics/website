<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Probabilistic Numerics


  | Optimization

</title>
<meta name="description" content="Quantifying Uncertainty in Computation.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Favicons -->

<link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png">
<link rel="manifest" href="/assets/img/favicons/site.webmanifest">
<link rel="mask-icon" href="/assets/img/favicons/safari-pinned-tab.svg" color="#28bab4">
<meta name="msapplication-TileColor" content="#28bab4">
<meta name="theme-color" content="#ffffff">


<!-- Styles -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/research/optimization/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->






<!-- Github -->
<script async defer src="https://buttons.github.io/buttons.js"></script>

    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->

    <header>

  <!-- Nav Bar -->
  <nav id="navbar"
    class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <div class="navbar-logo">
        <a href="/">
          <img src="/assets/img/logos/pn-logo-dark-txtright.svg">
        </a>
      </div>
      
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav"
        aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              About
              
            </a>
          </li>
          
          <!-- Blog -->
          <!-- <li class="nav-item ">
            <a class="nav-link" href="/blog/">
              blog
              
            </a>
          </li> -->
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item dropdown ">
            <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown"
              aria-haspopup="true" aria-expanded="false">
              Research
              
            </a>
            <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown">
              
              
              <a class="dropdown-item" href="/research/general/">General</a>
              
              
              
              <div class="dropdown-divider"></div>
              
              
              
              <a class="dropdown-item" href="/research/linear_algebra/">Linear Algebra</a>
              
              
              
              <a class="dropdown-item" href="/research/quadrature/">Quadrature</a>
              
              
              
              <a class="dropdown-item" href="/research/optimization/">Optimization</a>
              
              
              
              <a class="dropdown-item" href="/research/ode/">Ordinary Differential Equations</a>
              
              
              
              <a class="dropdown-item" href="/research/pde/">Partial Differential Equations</a>
              
              
              
              <div class="dropdown-divider"></div>
              
              
              
              <a class="dropdown-item" href="/research/other/">Other</a>
              
              
            </div>
          </li>
          
          
          
          
          
          <li class="nav-item ">
            <a class="nav-link" href="/code/">
              Code
              
            </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
            <a class="nav-link" href="/meetings/">
              Meetings
              
            </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
            <a class="nav-link" href="/teaching/">
              Teaching
              
            </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
            <a class="nav-link" href="/textbooks/">
              Textbooks
              
            </a>
          </li>
          
          
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">Optimization</h1>
    <p class="post-description"></p>
  </header>

  <article>
    <h3 id="local-optimization">Local Optimization</h3>

<div class="publications">
<h2 class="bibliography">2018</h2>
<ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="2018arXiv180704594D" class="col-sm-8">
    
      <div class="title">The Incremental Proximal Method: A Probabilistic Perspective</div>
      <div class="author">
        
          
          
          
          

          
            
              
                
                  Deniz Akyildiz, Ã–.,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Elvira, V.,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  and Miguez, J.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>ArXiv e-prints</em>
      
      
        2018
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      <a href="https://arxiv.org/abs/1807.04594" class="btn btn-sm z-depth-0" role="button" target="_blank">link</a>
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In this work, we highlight a connection between the incremental proximal method and stochastic filters. We begin by showing that the proximal operators coincide, and hence can be realized with, Bayes updates. We give the explicit form of the updates for the linear regression problem and show that there is a one-to-one correspondence between the proximal operator of the least-squares regression and the Bayes update when the prior and the likelihood are Gaussian. We then carry out this observation to a general sequential setting: We consider the incremental proximal method, which is an algorithm for large-scale optimization, and show that, for a linear-quadratic cost function, it can naturally be realized by the Kalman filter. We then discuss the implications of this idea for nonlinear optimization problems where proximal operators are in general not realizable. In such settings, we argue that the extended Kalman filter can provide a systematic way for the derivation of practical procedures.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li></ol>
<h2 class="bibliography">2015</h2>
<ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="NIPS2015_5753" class="col-sm-8">
    
      <div class="title">Probabilistic Line Searches for Stochastic Optimization</div>
      <div class="author">
        
          
          
          
          
            
              
                
                
          

          
            
              
                
                  Mahsereci, Maren,
                
              
            
          
        
          
          
          
          
            
              
                
                
          

          
            
              
                
                  and <a href="https://uni-tuebingen.de/de/134782" target="_blank">Hennig, Philipp</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
      
        2015
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      <a href="http://papers.nips.cc/paper/5753-probabilistic-line-searches-for-stochastic-optimization" class="btn btn-sm z-depth-0" role="button" target="_blank">link</a>
    
    
    
    
    
      <a href="https://is.tuebingen.mpg.de/uploads_file/attachment/attachment/242/probLS.zip" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In deterministic optimization, line searches are a standard tool ensuring stability and efficiency. Where only stochastic gradients are available, no direct equivalent has so far been formulated, because uncertain gradients do not allow for a strict sequence of decisions collapsing the search space. We construct a probabilistic line search by combining the structure of existing deterministic methods with notions from Bayesian optimization. Our method retains a Gaussian process surrogate of the univariate optimization objective, and uses a probabilistic belief over the Wolfe conditions to monitor the descent. The algorithm has very low computational cost, and no user-controlled parameters. Experiments show that it effectively removes the need to define a learning rate for stochastic gradient descent.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li></ol>
<h2 class="bibliography">2013</h2>
<ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="hennig13_quasi_newton_method" class="col-sm-8">
    
      <div class="title">Quasi-Newton Methods â€“ a new direction</div>
      <div class="author">
        
          
          
          
          
            
              
                
                
          

          
            
              
                
                  <a href="https://uni-tuebingen.de/de/134782" target="_blank">Hennig, P.</a>,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  and Kiefel, M.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Journal of Machine Learning Research</em>
      
      
        2013
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Four decades after their invention, quasi-Newton methods are
                  still state of the art in unconstrained numerical
                  optimization. Although not usually interpreted thus, these
                  are learning algorithms that fit a local quadratic
                  approximation to the objective function. We show that many,
                  including the most popular, quasi-Newton methods can be
                  interpreted as approximations of Bayesian linear regression
                  under varying prior assumptions. This new notion elucidates
                  some shortcomings of classical algorithms, and lights the
                  way to a novel nonparametric quasi-Newton method, which is
                  able to make more efficient use of available information at
                  computational cost similar to its predecessors.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="StochasticNewton" class="col-sm-8">
    
      <div class="title">Fast Probabilistic Optimization from Noisy Gradients</div>
      <div class="author">
        
          
          
          
          
            
              
                
                
          

          
            
              Hennig, P.
            
          
        
      </div>

      <div class="periodical">
      
        <em>In International Conference on Machine Learning (ICML)</em>
      
      
        2013
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Stochastic gradient descent remains popular in large-scale
                  machine learning, on account of its very low computational
                  cost and robust- ness to noise. However, gradient descent is
                  only linearly efficient and not transformation
                  invariant. Scaling by a local measure can substantially
                  improve its performance. One natural choice of such a scale
                  is the Hessian of the objective function: Were it available,
                  it would turn linearly efficient gradient descent into the
                  quadratically efficient Newton-Raphson optimization. Existing
                  covariant methods, though, are either super-linearly
                  expensive or do not address noise. Generalising recent
                  results, this paper constructs a nonparametric Bayesian
                  quasi-Newton algorithm that learns gradient and Hessian from
                  noisy evaluations of the gradient. Importantly, the resulting
                  algorithm, like stochastic gradient descent, has cost linear
                  in the number of input dimensions</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li></ol>
<h2 class="bibliography">2012</h2>
<ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="HennigKiefel" class="col-sm-8">
    
      <div class="title">Quasi-Newton methods â€“ a new direction</div>
      <div class="author">
        
          
          
          
          
            
              
                
                
          

          
            
              
                
                  <a href="https://uni-tuebingen.de/de/134782" target="_blank">Hennig, P.</a>,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  and Kiefel, M.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In International Conference on Machine Learning (ICML)</em>
      
      
        2012
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Four decades after their invention, quasi- Newton methods are
                  still state of the art in unconstrained numerical
                  optimization. Al- though not usually interpreted thus, these
                  are learning algorithms that fit a local quadratic
                  approximation to the objective function. We show that many,
                  including the most popular, quasi-Newton methods can be
                  interpreted as approximations of Bayesian linear regression
                  under varying prior assumptions. This new notion elucidates
                  some shortcomings of clas- sical algorithms, and lights the
                  way to a novel nonparametric quasi-Newton method, which is
                  able to make more efficient use of available information at
                  computational cost similar to its predecessors.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li></ol>
</div>

<h3 id="global-optimization">Global Optimization</h3>

<div class="publications">
<h2 class="bibliography">2012</h2>
<ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="HennigS2012" class="col-sm-8">
    
      <div class="title">Entropy Search for Information-Efficient Global Optimization</div>
      <div class="author">
        
          
          
          
          
            
              
                
                
          

          
            
              
                
                  <a href="https://uni-tuebingen.de/de/134782" target="_blank">Hennig, P.</a>,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  and Schuler, CJ.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Journal of Machine Learning Research</em>
      
      
        2012
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      <a href="http://jmlr.csail.mit.edu/papers/v13/hennig12a.html" class="btn btn-sm z-depth-0" role="button" target="_blank">link</a>
    
    
    
    
    
      <a href="http://probabilistic-optimization.org/Global.html" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Contemporary global optimization algorithms are based on
                  local measures of utility, rather than a probability measure
                  over location and value of the optimum. They thus attempt to
                  collect low function values, not to learn about the
                  optimum. The reason for the absence of probabilistic global
                  optimizers is that the corresponding inference problem is
                  intractable in several ways. This paper develops desiderata
                  for probabilistic optimization algorithms, then presents a
                  concrete algorithm which addresses each of the computational
                  intractabilities with a sequence of approximations and
                  explicitly adresses the decision problem of maximizing
                  information gain from each evaluation. </p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li></ol>
<h2 class="bibliography">1978</h2>
<ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="mockus1978application" class="col-sm-8">
    
      <div class="title">The application of Bayesian methods for seeking the extremum</div>
      <div class="author">
        
          
          
          
          

          
            
              
                
                  Mockus, Jonas,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Tiesis, Vytautas,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  and Zilinskas, Antanas
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Towards global optimization</em>
      
      
        1978
      
      </div>
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li></ol>
</div>

  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="sticky-bottom mt-5">
  <div class="container">

    <!-- 
    <div class="watermark">
      <img src="www.probabilistic-numerics.org/assets/img/logos/pn-watermark.svg">
    </div>
     -->

    <div class="social">
      <div class="contact-icons">
        

<a href="https://scholar.google.com/scholar?q=probabilistic+numerics" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>


<a href="https://github.com/probabilistic-numerics" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>












      </div>
    </div>
    Open an <a href="https://github.com/probabilistic-numerics/website/issues">issue</a> or <a href="https://github.com/probabilistic-numerics/website/pulls">pull request</a> on GitHub to get in touch or suggest changes to this site.

    <br><br>
    &copy; Copyright 2024
    <br>
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> and the <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted on <a href="https://github.com/probabilistic-numerics/website" target="_blank" rel="noopener">GitHub</a>.

    
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
